---
title: "Settings"
description: "Complete control center for your ClaraVerse workspace"
category: "features"
order: 8
lastUpdated: "2025-09-06"
contributors: ["badboysm890"]
---

# ‚öôÔ∏è Settings

<img src="https://raw.githubusercontent.com/badboysm890/ClaraVerse/935d0659b468f2d896f7acf2878725c35500cbe6/public/mascot/Settings.png" alt="Clara managing settings and configurations" width="400" />

**Your mission control center for the entire ClaraVerse ecosystem.**

Settings is where you take complete control of your AI workspace‚Äîfrom AI providers and local models to service management and system updates. This isn't just configuration; it's the command center that makes everything work exactly how you want it.

## üéõÔ∏è What's Inside Settings

### üß† **AI Services**
**Your AI Provider Hub**
- **Multiple Provider Support**: OpenAI, Claude, local Ollama, custom OpenAI-compatible APIs
- **Provider Management**: Add, test, edit, and delete AI providers with one click
- **Automatic Testing**: Built-in connection testing to verify your API keys and endpoints
- **Primary Provider Selection**: Choose your default AI provider for Clara Assistant
- **Custom Endpoints**: Support for any OpenAI-compatible API (local or cloud)

### üñ•Ô∏è **Local Models**
**Complete Local AI Management**
- **Clara Core Models**: Manage models optimized for your specific hardware
- **Ollama Integration**: Browse and manage your Ollama model collection
- **Model Information**: See model size, format, quantization, and parameters
- **GPU Diagnostics**: Real-time GPU monitoring and performance metrics
- **Storage Management**: Track model storage usage and cleanup tools
- **Hardware Optimization**: Automatic model selection based on your system specs

### üîß **Services**
**Your Docker & Service Command Center**
- **Unified Service Manager**: Control all ClaraVerse services from one interface
- **Docker Container Management**: Start, stop, and monitor Docker containers
- **Service Auto-Discovery**: Automatically detect running services and their ports
- **Health Monitoring**: Real-time status for N8N, Python backends, Clara Core
- **ComfyUI Management**: Control your image generation service
- **LlamaSwap Integration**: Manage local model serving

### üé® **Preferences**
**Personalize Your Experience**
- **Theme Control**: Light, dark, or system-adaptive themes
- **Timezone Configuration**: Set your timezone for accurate scheduling
- **Interface Customization**: Tailor the UI to your preferences
- **Wallpaper Settings**: Custom backgrounds for your workspace
- **Accessibility Options**: Configure for optimal usability

### üë§ **Profile**
**Your Digital Identity**
- **Personal Information**: Name, bio, and preferences for Clara's memory
- **Usage Analytics**: Track your productivity and feature usage
- **Privacy Controls**: Manage data retention and sharing preferences
- **Export Options**: Backup your settings and configurations

### üì¶ **Updates**
**Stay Current & Secure**
- **Automatic Update Checking**: Stay informed about new ClaraVerse releases
- **Llama.cpp Updates**: Keep your local model engine up to date
- **Alpha Features**: Early access to cutting-edge features
- **Release Notes**: See what's new in each update
- **One-Click Updates**: Seamless updating process

### üíª **Export as Code**
**Transform Workflows into Applications**
- **JavaScript Export**: Convert your agent workflows into standalone JavaScript modules
- **SDK Integration**: Generate code compatible with Clara Flow SDK
- **Custom Applications**: Build web apps and services using your workflows
- **TypeScript Support**: Generate type-safe code for professional development
- **Microservices Ready**: Create deployable workflow services

## üîó Integration with ClaraVerse Ecosystem

Settings isn't just configuration‚Äîit's the foundation that makes every other feature work seamlessly.

### ü§ñ **Clara Assistant Integration**
- **Provider Configuration**: Set up AI providers that Clara uses for responses
- **Model Selection**: Choose which local or cloud models Clara accesses
- **Memory Settings**: Configure how Clara remembers and learns from interactions

### ü§ñ **Agents Integration**
- **Service Dependencies**: Ensure all required services are running for agent workflows
- **Model Access**: Configure which AI models agents can use in their nodes
- **Export Workflows**: Turn agent workflows into standalone applications

### üìö **Notebooks Integration**
- **Backend Configuration**: Set up the Python services that power RAG functionality
- **Storage Settings**: Configure where notebook data and embeddings are stored
- **Processing Options**: Choose embedding and LLM providers for notebook intelligence

### üé® **ImageGen Integration**
- **ComfyUI Configuration**: Set up and manage your image generation backend
- **Model Management**: Configure which Stable Diffusion models are available
- **Storage Optimization**: Manage model storage and download settings

### üíª **LumaUI Integration**
- **Development Environment**: Configure the web development stack
- **AI Model Access**: Set up which AI providers LumaUI can use for code generation
- **Export Options**: Configure code generation and deployment settings

### ‚ö° **N8N Integration**
- **Service Management**: Start and configure N8N for workflow automation
- **Authentication**: Set up external service connections securely
- **Webhook Configuration**: Manage webhook endpoints for Clara tool integration

## üîÑ Real Configuration Examples

### **Perfect Local Setup:**
1. **Services Tab**: Start Clara Core, N8N, and Python services
2. **Local Models**: Download and configure your preferred local models
3. **AI Services**: Set Clara Core as primary provider for privacy
4. **Preferences**: Set dark theme and your timezone
5. **Profile**: Configure your information for Clara's memory

### **Hybrid Cloud Setup:**
1. **AI Services**: Add OpenAI for advanced tasks, local models for privacy
2. **Services**: Configure selective service startup
3. **Local Models**: Keep lightweight models for basic tasks
4. **Export Code**: Generate applications that use both local and cloud AI

### **Team/Enterprise Setup:**
1. **AI Services**: Configure shared OpenAI-compatible endpoints
2. **Services**: Set up centralized service management
3. **Updates**: Enable automatic updates and alpha features
4. **Export Code**: Generate deployable microservices from workflows

## üí° Pro Tips

- **Start with Local Models**: Configure local AI first for complete privacy
- **Test Providers**: Always test API connections before setting as primary
- **Monitor Resources**: Use GPU diagnostics to optimize model selection
- **Regular Updates**: Keep llama.cpp and ClaraVerse updated for best performance
- **Export Workflows**: Turn successful agent workflows into reusable code modules
- **Service Health**: Regularly check service status for optimal performance

## Getting Started

1. **Open Settings**: Click the gear icon in the sidebar
2. **Start Services**: Go to Services tab and start required Docker containers
3. **Add AI Provider**: Configure at least one AI provider in AI Services
4. **Set Preferences**: Customize theme and timezone in Preferences
5. **Test Everything**: Use the test buttons to verify all connections work

Settings is your control room for the entire ClaraVerse ecosystem. Master it, and you master your AI workspace.

---
**The foundation of your AI empire starts here. Configure once, create forever.**
